{"cells":[{"cell_type":"markdown","source":["In this Notebook, we'll train a simple image classifier & deploy it as a web API.\n\nWe'll create an experiment.  We'll create a run within that experiment and train a model.\n\nWe'll then register the model.\n\nWe'll do that for two versions of the model.\n\nThen we'll deploy the last version."],"metadata":{}},{"cell_type":"code","source":["# import the Workspace class and check the azureml SDK version\nimport azureml.core\nfrom azureml.core import Workspace\n\nworkspace = Workspace.from_config(path=\"/dbfs/aml_config/config.json\")\n\nprint('Workspace name: ' + workspace.name, \n      'Azure region: ' + workspace.location, \n      'Subscription id: ' + workspace.subscription_id, \n      'Resource group: ' + workspace.resource_group, sep = '\\n')\n\n# check core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import os\nimport urllib.request\n\nos.makedirs('./data', exist_ok = True)\n\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename='./data/train-images.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename='./data/train-labels.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename='./data/test-images.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename='./data/test-labels.gz')"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License.\n\nimport gzip\nimport numpy as np\nimport struct\n\n# load compressed MNIST gz files and return numpy arrays\ndef load_data(filename, label=False):\n    with gzip.open(filename) as gz:\n        struct.unpack('I', gz.read(4))\n        n_items = struct.unpack('>I', gz.read(4))\n        if not label:\n            n_rows = struct.unpack('>I', gz.read(4))[0]\n            n_cols = struct.unpack('>I', gz.read(4))[0]\n            res = np.frombuffer(gz.read(n_items[0] * n_rows * n_cols), dtype=np.uint8)\n            res = res.reshape(n_items[0], n_rows * n_cols)\n        else:\n            res = np.frombuffer(gz.read(n_items[0]), dtype=np.uint8)\n            res = res.reshape(n_items[0], 1)\n    return res\n\n# one-hot encode a 1-D array\ndef one_hot_encode(array, num_of_classes):\n    return np.eye(num_of_classes)[array.reshape(-1)]"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the model converge faster.\nX_train = load_data('./data/train-images.gz', False) / 255.0\ny_train = load_data('./data/train-labels.gz', True).reshape(-1)\n\nX_test = load_data('./data/test-images.gz', False) / 255.0\ny_test = load_data('./data/test-labels.gz', True).reshape(-1)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["y_train"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from azureml.core import Experiment\n\nexperiment_name = 'simple-classifier'\nexperiment = Experiment(workspace=workspace, name=experiment_name)\nrun = experiment.start_logging()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["y_hat = clf.predict(X_test)\nsuccessRate = np.average(y_hat == y_test)\n\nrun.log(\"Success Rate\", successRate)\nprint(successRate)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["import pickle\n\n# Write model to file\nwith open(\"LogisticRegression\", \"wb\") as f:\n  pickle.dump(clf,f)\nrun.upload_file(name = 'outputs/LogisticRegression', path_or_stream = 'LogisticRegression')"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Complete tracking and get link to details\nrun.complete()\nprint(\"Run completed\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["model = run.register_model(model_name = \"image-classifier\", model_path = \"outputs/LogisticRegression\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["rootRun = experiment.start_logging()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Regularization Rates\nregs = [0.0001, 0.01, 0.1, 1, 2]\n\nrootRun.log(\"Number of regularization rates\", str(len(regs)))\nbestSuccessRate=0\nbestModel=None\n\nfor reg in regs:\n  with rootRun.child_run(\"reg-\" + str(reg)) as subRun:\n    #  Train\n    clf = LogisticRegression(C=reg)\n    clf.fit(X_train, y_train)\n    \n    #  Test\n    y_hat = clf.predict(X_test)\n    successRate = np.average(y_hat == y_test)\n\n    #  Log results\n    subRun.log(\"Regularization rate\", reg)\n    subRun.log(\"Success Rate\", successRate)\n    rootRun.log(\"Success Rate\", successRate)\n    print('Regularization rate: ' + str(reg),\n          'Success Rate: ' + str(successRate),\n          sep='\\n')\n    \n    # Write model to file\n    with open(\"LogisticRegression\", \"wb\") as f:\n      pickle.dump(clf,f)\n    subRun.upload_file(name = 'outputs/LogisticRegression', path_or_stream = 'LogisticRegression')\n    \n    # Keep best model\n    if(successRate>bestSuccessRate):\n      bestSuccessRate=successRate\n      bestModel=clf"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Write model to file\nwith open(\"LogisticRegression\", \"wb\") as f:\n  pickle.dump(bestModel,f)\nrootRun.upload_file(name = 'outputs/LogisticRegression', path_or_stream = 'LogisticRegression')\n\n# Log metrics\nrootRun.log(\"Best Success Rate\", bestSuccessRate)\nrootRun.log(\"Best Regularization Rate\", bestModel.C)\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Complete tracking and get link to details\nrootRun.complete()\nprint(\"Root Run completed\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["model = rootRun.register_model(model_name = \"image-classifier\", model_path = \"outputs/LogisticRegression\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Let's create a scoring script that computes an area of a circle, given the estimate within the pi_estimate model. The scoring script consists of two parts: \n\n * The *init* method that loads the model. You can retrieve registered model using *Model.get_model_path* method. \n * The *run* method that gets invoked when you call the web service. It computes the area of a circle using the well-known $area = \\pi*radius^2$ formula. The inputs and outputs are passed as json-formatted strings."],"metadata":{}},{"cell_type":"code","source":["%%writefile score.py\nimport json\nimport numpy as np\nimport os\nimport pickle\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression\n\nfrom azureml.core.model import Model\n\ndef init():\n    global model\n    # retreive the path to the model file using the model name\n    model_path = Model.get_model_path('image-classifier')\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)['data'])\n    print(\"Input:  \" + str(data))\n    # make prediction\n    y_hat = model.predict(data)\n    print (\"Ouptut:  \" + str(y_hat))\n    # you can return any data type as long as it is JSON-serializable\n    return y_hat.tolist()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"scikit-learn\")\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())\nwith open(\"myenv.yml\",\"r\") as f:\n    print(f.read())"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=1, \n                                               tags={\"data\": \"MNIST\",  \"method\" : \"sklearn\"}, \n                                               description='Predict MNIST with sklearn')"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from azureml.core.webservice import Webservice\nfrom azureml.core.image import ContainerImage\n\n# configure the image\nimage_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                                                  runtime=\"python\", \n                                                  conda_file=\"myenv.yml\")\n\n# Deploy\nservice = Webservice.deploy_from_model(workspace=workspace,\n                                       name='image-classifier-svc',\n                                       deployment_config=aciconfig,\n                                       models=[model],\n                                       image_config=image_config)\n\nservice.wait_for_deployment(show_output=True)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["print(service.scoring_uri)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["import json\n\n# find 30 random samples from test set\nn = 30\nsample_indices = np.random.permutation(X_test.shape[0])[0:n]\n\ntest_samples = json.dumps({\"data\": X_test[sample_indices].tolist()})\ntest_samples = bytes(test_samples, encoding='utf8')\n\n# predict using the deployed model\nresult = service.run(input_data=test_samples)\n\nresult"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":25}],"metadata":{"name":"2-Deploy image classifier (MNIST)","notebookId":2437886971199669},"nbformat":4,"nbformat_minor":0}
